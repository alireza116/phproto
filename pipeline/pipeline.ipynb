{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import textacy\n",
    "import re\n",
    "from textacy.preprocessing import remove_punctuation, replace_emojis, replace_urls\n",
    "import textacy.vsm \n",
    "from textacy.vsm import Vectorizer\n",
    "import textacy.tm\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "import pandas as pd\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "import multiprocessing as mp\n",
    "import ast\n",
    "from twitteremotion.emotion_predictor import EmotionPredictor\n",
    "import preprocessor as p\n",
    "import json\n",
    "p.set_options(p.OPT.URL, p.OPT.EMOJI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"rawDAta/twitter_Oct20-Dec25_geocoded_Feb10YH.csv\"\n",
    "outPath = \"processedData/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path,encoding=\"iso-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stops = [\"t\",\"co\",\"s\", \"t co\",\"g\",'s   t co',\"https\",\"htt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntities(tweet):\n",
    "  blob = TextBlob(tweet)\n",
    "  result = []\n",
    "  for sent in blob.sentences:\n",
    "    sent = str(sent)\n",
    "    sentence = Sentence(sent)\n",
    "    tagger.predict(sentence)\n",
    "    entities = sentence.to_dict(tag_type='ner')\n",
    "    for ent in entities[\"entities\"]:\n",
    "      result.append([ent[\"text\"],ent[\"labels\"]])\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = p.clean(text)\n",
    "  text = remove_punctuation(text)\n",
    "  text = replace_emojis(text)\n",
    "  text = replace_urls(text)\n",
    "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "  text = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''',\"\",text)\n",
    "  text = re.sub(r'[\\d-]',\"\",text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darts/Desktop/karduni/phproto/pipeline/venv/lib/python3.8/site-packages/h5py/__init__.py:72: UserWarning: h5py is running against HDF5 1.10.4 when it was built against 1.8.4, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "emot_model = EmotionPredictor(classification='ekman', setting='mc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = emot_model.predict_probabilities(df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emotion in [\"Anger\",\t\"Disgust\",\t'Fear',\t\"Joy\",\t\"Sadness\",\t\"Surprise\"]:\n",
    "  df[emotion] = predictions[emotion]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-16 22:39:05,755 loading file /Users/darts/.flair/models/en-ner-conll03-v0.4.pt\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>text</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>source</th>\n",
       "      <th>truncated</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>...</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Anger</th>\n",
       "      <th>Disgust</th>\n",
       "      <th>Fear</th>\n",
       "      <th>Joy</th>\n",
       "      <th>Sadness</th>\n",
       "      <th>Surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-10-15 15:12:20+00:00</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>@feverist_ à¸à¸¥à¸±à¸à¸¢à¸±à¸</td>\n",
       "      <td>[11, 18]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>6.920813e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>feverist</td>\n",
       "      <td>0.005877</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.019198</td>\n",
       "      <td>0.747915</td>\n",
       "      <td>0.027629</td>\n",
       "      <td>0.197262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-10-15 15:48:56+00:00</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>@gulfeverindia @gulfkanawut ðà¸£à¸à¸à¸§à¸...</td>\n",
       "      <td>[28, 48]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.230000e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gulfeverindia  gulfkanawut</td>\n",
       "      <td>0.010276</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>0.022733</td>\n",
       "      <td>0.553315</td>\n",
       "      <td>0.045737</td>\n",
       "      <td>0.364985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-10-15 15:49:56+00:00</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>@gulfeverindia @gulfkanawut Please Can you del...</td>\n",
       "      <td>[28, 58]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.230000e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gulfeverindia  gulfkanawut Please Can you del...</td>\n",
       "      <td>0.097684</td>\n",
       "      <td>0.049475</td>\n",
       "      <td>0.452972</td>\n",
       "      <td>0.048981</td>\n",
       "      <td>0.217681</td>\n",
       "      <td>0.133206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-10-15 16:38:45+00:00</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>@A79B97G97 @gulfeverindia @gulfkanawut à¹à¸«à...</td>\n",
       "      <td>[39, 63]</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>3.490435e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABG  gulfeverindia  gulfkanawut</td>\n",
       "      <td>0.011113</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.582208</td>\n",
       "      <td>0.031821</td>\n",
       "      <td>0.330358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-10-14 05:55:16+00:00</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>1.320000e+18</td>\n",
       "      <td>Thailand reports 9 new Covid-19 cases, quarant...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thailand reports  new Covid  cases  quarantine...</td>\n",
       "      <td>0.026948</td>\n",
       "      <td>0.018163</td>\n",
       "      <td>0.122829</td>\n",
       "      <td>0.285686</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>0.510258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  created_at            id        id_str  \\\n",
       "0  2020-10-15 15:12:20+00:00  1.320000e+18  1.320000e+18   \n",
       "1  2020-10-15 15:48:56+00:00  1.320000e+18  1.320000e+18   \n",
       "2  2020-10-15 15:49:56+00:00  1.320000e+18  1.320000e+18   \n",
       "3  2020-10-15 16:38:45+00:00  1.320000e+18  1.320000e+18   \n",
       "4  2020-10-14 05:55:16+00:00  1.320000e+18  1.320000e+18   \n",
       "\n",
       "                                                text display_text_range  \\\n",
       "0                   @feverist_ à¸à¸¥à¸±à¸à¸¢à¸±à¸           [11, 18]   \n",
       "1  @gulfeverindia @gulfkanawut ðà¸£à¸à¸à¸§à¸...           [28, 48]   \n",
       "2  @gulfeverindia @gulfkanawut Please Can you del...           [28, 58]   \n",
       "3  @A79B97G97 @gulfeverindia @gulfkanawut à¹à¸«à...           [39, 63]   \n",
       "4  Thailand reports 9 new Covid-19 cases, quarant...                NaN   \n",
       "\n",
       "                                              source  truncated  \\\n",
       "0  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "2  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...      False   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...      False   \n",
       "\n",
       "   in_reply_to_status_id  in_reply_to_status_id_str  in_reply_to_user_id  ...  \\\n",
       "0           1.320000e+18               1.320000e+18         6.920813e+07  ...   \n",
       "1           1.320000e+18               1.320000e+18         1.230000e+18  ...   \n",
       "2           1.320000e+18               1.320000e+18         1.230000e+18  ...   \n",
       "3           1.320000e+18               1.320000e+18         3.490435e+09  ...   \n",
       "4                    NaN                        NaN                  NaN  ...   \n",
       "\n",
       "   quoted_status_permalink extended_tweet withheld_in_countries  \\\n",
       "0                      NaN            NaN                   NaN   \n",
       "1                      NaN            NaN                   NaN   \n",
       "2                      NaN            NaN                   NaN   \n",
       "3                      NaN            NaN                   NaN   \n",
       "4                      NaN            NaN                   NaN   \n",
       "\n",
       "                                          clean_text     Anger   Disgust  \\\n",
       "0                                          feverist   0.005877  0.002119   \n",
       "1                         gulfeverindia  gulfkanawut  0.010276  0.002954   \n",
       "2   gulfeverindia  gulfkanawut Please Can you del...  0.097684  0.049475   \n",
       "3                    ABG  gulfeverindia  gulfkanawut  0.011113  0.003875   \n",
       "4  Thailand reports  new Covid  cases  quarantine...  0.026948  0.018163   \n",
       "\n",
       "       Fear       Joy   Sadness  Surprise  \n",
       "0  0.019198  0.747915  0.027629  0.197262  \n",
       "1  0.022733  0.553315  0.045737  0.364985  \n",
       "2  0.452972  0.048981  0.217681  0.133206  \n",
       "3  0.040625  0.582208  0.031821  0.330358  \n",
       "4  0.122829  0.285686  0.036117  0.510258  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n",
      "650\n",
      "700\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "# ents = [getEntities(text) for text in df[\"text\"]]\n",
    "ents= []\n",
    "for i,text in enumerate(df[\"text\"]):\n",
    "    ents.append(getEntities(text))\n",
    "    if i%50==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ents\"] = ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllLocEnts = []\n",
    "for entList in df[\"ents\"]:\n",
    "  locEnts = []\n",
    "  for ent in entList:\n",
    "    entText = ent[0]\n",
    "    entLabel = ent[1][0].to_dict()\n",
    "    if entLabel[\"value\"] == \"LOC\":\n",
    "      locEnts.append(entText)\n",
    "  AllLocEnts.append(locEnts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"locEnts\"] = AllLocEnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"entity\"] = df[\"locEnts\"].apply(lambda x: x[0] if len(x)>0 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 3\n",
    "en = textacy.load_spacy_lang(\"enn\") #, disable=(\"parser\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = textacy.corpus.Corpus(lang=en, data=[i for i in df[\"clean_text\"]]) #data=texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = (doc._.to_terms_list(entities=False, as_strings=True, normalize = \"lower\") for doc in corpus) #  \n",
    "tokenized_docs = ([i for i in token_list if i not in other_stops and len(i)>2] for token_list in tokenized_docs)\n",
    "# do tf-idf\n",
    "vectorizer = Vectorizer(norm=\"l2\", apply_idf=True, max_df=0.95) # , vocabulary_terms = s, max_df=0.95, min_df=5, norm=\"l2\", apply_idf=True, \n",
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = textacy.tm.TopicModel(\"nmf\", n_topics=20)\n",
    "model.fit(doc_term_matrix)\n",
    "doc_topic_matrix = model.transform(doc_term_matrix)\n",
    "doc_topic_matrix.shape\n",
    "topicTermsDict = {}\n",
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, top_n=20):\n",
    "    topicTermsDict[topic_idx] = list(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = list(model.top_doc_topics(doc_topic_matrix=doc_topic_matrix,docs=-1,top_n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"topic_id\"] = [i[1][0] for i in doc_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ['covid',\n",
       "  'coronavirus',\n",
       "  'mask',\n",
       "  'tnamcot',\n",
       "  'timeli',\n",
       "  'japan covid',\n",
       "  'vaccine',\n",
       "  'gym',\n",
       "  'gulfkanawut',\n",
       "  'gusta',\n",
       "  'gusta tu',\n",
       "  '||years ago',\n",
       "  'gulfeverindia',\n",
       "  'h nam',\n",
       "  'h ngay',\n",
       "  'h ngay c',\n",
       "  'h ni',\n",
       "  'h ni ghi',\n",
       "  'h ni lp',\n",
       "  'h ni nghi'],\n",
       " 1: ['feverist',\n",
       "  'wori',\n",
       "  'aoftronaut',\n",
       "  'center',\n",
       "  'perfect',\n",
       "  'iamipho',\n",
       "  'youtuber',\n",
       "  'viewskk',\n",
       "  'panya',\n",
       "  'bancha panya',\n",
       "  'bancha',\n",
       "  'misakasiamdream',\n",
       "  'ittisakg',\n",
       "  'paipw',\n",
       "  'suddenota',\n",
       "  'joysitck',\n",
       "  'writenoonz',\n",
       "  'aoftronaut matsuri',\n",
       "  'matsuri',\n",
       "  'marclamonthill'],\n",
       " 2: ['timeline',\n",
       "  '||years ago',\n",
       "  'gulfeverindia',\n",
       "  'gusta',\n",
       "  'gusta tu',\n",
       "  'gym',\n",
       "  'gym need',\n",
       "  'h nam',\n",
       "  'h ngay',\n",
       "  'h ngay c',\n",
       "  'h ni',\n",
       "  'h ni ghi',\n",
       "  'h ni lp',\n",
       "  'h ni nghi',\n",
       "  'ha bnh',\n",
       "  'ha bnh v',\n",
       "  'ha dejado',\n",
       "  'gulfkanawut',\n",
       "  'guidelines with world',\n",
       "  'ha v'],\n",
       " 3: ['feverth',\n",
       "  'ifyouwantme',\n",
       "  'spamfever',\n",
       "  'plyfever',\n",
       "  'want',\n",
       "  'views',\n",
       "  'baimonfever',\n",
       "  'siammatsuri',\n",
       "  'shot',\n",
       "  'undergroundfever',\n",
       "  'ifyouwantmefever',\n",
       "  'baifernfever',\n",
       "  'westgate',\n",
       "  'froyfever',\n",
       "  'beambeamfever mv',\n",
       "  'central',\n",
       "  'central plaza westgate',\n",
       "  'central plaza',\n",
       "  'plaza',\n",
       "  'plaza westgate'],\n",
       " 4: ['vit nam c',\n",
       "  'nam c',\n",
       "  'vit nam',\n",
       "  'bnh',\n",
       "  'vit',\n",
       "  'nam',\n",
       "  'bnh nhn',\n",
       "  'nhn',\n",
       "  'thm',\n",
       "  'chiu',\n",
       "  'mc covid',\n",
       "  'covid',\n",
       "  'mc mi covid',\n",
       "  'mc mi',\n",
       "  'ngi',\n",
       "  'mi covid',\n",
       "  'tr v t',\n",
       "  'tr v',\n",
       "  'v t',\n",
       "  'ngi tr'],\n",
       " 5: ['covid report',\n",
       "  'shows',\n",
       "  'thailand shows',\n",
       "  'report',\n",
       "  'coronavirus cases',\n",
       "  'new',\n",
       "  'cases',\n",
       "  'zero deaths',\n",
       "  'deaths',\n",
       "  'today s',\n",
       "  'zero',\n",
       "  'state',\n",
       "  'quarantine',\n",
       "  'amp',\n",
       "  'cases from state',\n",
       "  'quarantine and zero',\n",
       "  'statequarantine',\n",
       "  'cases in state',\n",
       "  'stil',\n",
       "  'coronavirus case'],\n",
       " 6: ['fever',\n",
       "  'shot',\n",
       "  'lookyee',\n",
       "  'lookyee fever',\n",
       "  'want',\n",
       "  'shot ~',\n",
       "  'shot fever',\n",
       "  'want meofficial',\n",
       "  'want meofficial mv',\n",
       "  'meofficial mv',\n",
       "  'meofficial',\n",
       "  'yellow',\n",
       "  'fridayokaeri',\n",
       "  'fridayokaeri fever',\n",
       "  'yellow fever',\n",
       "  'sick',\n",
       "  'feel sick',\n",
       "  'fever follower',\n",
       "  'follower',\n",
       "  'harumiizz'],\n",
       " 7: ['ihealthy',\n",
       "  'ihealthy covid',\n",
       "  'covid',\n",
       "  'mumimm',\n",
       "  'imimimzi ihealthy covid',\n",
       "  'jakkrittomtom ihealthy covid',\n",
       "  'ttxswsy ihealthy covid',\n",
       "  'snowyphone ihealthy covid',\n",
       "  'snowyphone ihealthy',\n",
       "  'snowyphone',\n",
       "  'ttxswsy ihealthy',\n",
       "  'imimimzi',\n",
       "  'ttxswsy',\n",
       "  'natnpdo',\n",
       "  'jakkrittomtom',\n",
       "  'natnpdo ihealthy',\n",
       "  'natnpdo ihealthy covid',\n",
       "  'ppoopea',\n",
       "  'ppoopea ihealthy',\n",
       "  'ppoopea ihealthy covid'],\n",
       " 8: ['safe and informed',\n",
       "  'staying safe',\n",
       "  'informed on twitter',\n",
       "  'informed',\n",
       "  'staying',\n",
       "  'safe',\n",
       "  'gusta tu',\n",
       "  'gulfeverindia',\n",
       "  'gulfkanawut',\n",
       "  'gusta',\n",
       "  'gym need',\n",
       "  'gym',\n",
       "  'guidelines',\n",
       "  'h nam',\n",
       "  'h ngay',\n",
       "  'h ngay c',\n",
       "  'h ni',\n",
       "  'h ni ghi',\n",
       "  'guidelines with world',\n",
       "  '||years ago'],\n",
       " 9: ['virus',\n",
       "  'virus covid',\n",
       "  'm virus covid',\n",
       "  'm virus',\n",
       "  'bu c cc',\n",
       "  'c cc cp',\n",
       "  'cc cp',\n",
       "  'c cc',\n",
       "  'bu c',\n",
       "  'covid',\n",
       "  'abc',\n",
       "  'ok virus',\n",
       "  'visa',\n",
       "  'lilysexy',\n",
       "  'canada ok',\n",
       "  'virus bu c',\n",
       "  'virus bu',\n",
       "  'canada',\n",
       "  'ok virus covid',\n",
       "  'abc m virus'],\n",
       " 10: ['ghi nhn',\n",
       "  'ghi',\n",
       "  'vit nam ghi',\n",
       "  'nam ghi nhn',\n",
       "  'nam ghi',\n",
       "  'mi covid',\n",
       "  'mc mi',\n",
       "  'ghi nhn thm',\n",
       "  'nhn thm',\n",
       "  'vit',\n",
       "  'nam',\n",
       "  'mc mi covid',\n",
       "  'nhn',\n",
       "  'thm',\n",
       "  'u l',\n",
       "  'cnh',\n",
       "  'nhp',\n",
       "  'covid',\n",
       "  'vit nam',\n",
       "  'chiu'],\n",
       " 11: ['headache',\n",
       "  'headache lol',\n",
       "  'lfc',\n",
       "  'headache for jk',\n",
       "  'chocolateelixir nah',\n",
       "  'chocolateelixir',\n",
       "  'lol',\n",
       "  'everyday',\n",
       "  'headache everyday',\n",
       "  'lot',\n",
       "  'sleep',\n",
       "  'feel',\n",
       "  'help',\n",
       "  'feel a lot',\n",
       "  'lot of headache',\n",
       "  'fkm i feel',\n",
       "  'fkm',\n",
       "  'headache and common',\n",
       "  'common',\n",
       "  'common cold'],\n",
       " 12: ['hong',\n",
       "  'kong',\n",
       "  'residents',\n",
       "  'coronavirus pandemic',\n",
       "  'hong kong residents',\n",
       "  'kong residents',\n",
       "  'pandemic than people',\n",
       "  'jobs during coronavirus',\n",
       "  'losing',\n",
       "  'losing their jobs',\n",
       "  'residents more afraid',\n",
       "  'afraid of losing',\n",
       "  'jobs',\n",
       "  'afraid',\n",
       "  'pandemic',\n",
       "  'coronavirus',\n",
       "  'thk |',\n",
       "  '| nroom',\n",
       "  'nroom',\n",
       "  'thk'],\n",
       " 13: ['koh',\n",
       "  'samui',\n",
       "  'single case',\n",
       "  'tao',\n",
       "  'infections in koh',\n",
       "  'consecutive',\n",
       "  'consecutive days',\n",
       "  'phangan',\n",
       "  'koh samui',\n",
       "  'koh phangan',\n",
       "  'case of covid',\n",
       "  'single',\n",
       "  'case',\n",
       "  'days',\n",
       "  'infections',\n",
       "  'recently',\n",
       "  'recently returned',\n",
       "  'samui covid',\n",
       "  'confirmed case',\n",
       "  'confirmed'],\n",
       " 14: ['nhp cnh',\n",
       "  'cch ly',\n",
       "  'cch',\n",
       "  'cnh',\n",
       "  'nhp',\n",
       "  'ngay',\n",
       "  'ly ngay',\n",
       "  'khi nhp',\n",
       "  'khi nhp cnh',\n",
       "  'khi',\n",
       "  'cch ly ngay',\n",
       "  'c cch',\n",
       "  'c cch ly',\n",
       "  'ngay khi',\n",
       "  'ly ngay khi',\n",
       "  'ngay khi nhp',\n",
       "  'thm',\n",
       "  'mc covid',\n",
       "  'sau',\n",
       "  'ngi nhp'],\n",
       " 15: ['usa',\n",
       "  '||years ago',\n",
       "  'h ngay c',\n",
       "  'gusta',\n",
       "  'gusta tu',\n",
       "  'gym',\n",
       "  'gym need',\n",
       "  'h nam',\n",
       "  'h ngay',\n",
       "  'h ni',\n",
       "  'gulfeverindia',\n",
       "  'h ni ghi',\n",
       "  'h ni lp',\n",
       "  'h ni nghi',\n",
       "  'ha bnh',\n",
       "  'ha bnh v',\n",
       "  'ha dejado',\n",
       "  'ha dejado ms',\n",
       "  'gulfkanawut',\n",
       "  'guidelines with world'],\n",
       " 16: ['thaipbs',\n",
       "  '||years ago',\n",
       "  'h ngay c',\n",
       "  'gusta',\n",
       "  'gusta tu',\n",
       "  'gym',\n",
       "  'gym need',\n",
       "  'h nam',\n",
       "  'h ngay',\n",
       "  'h ni',\n",
       "  'gulfeverindia',\n",
       "  'h ni ghi',\n",
       "  'h ni lp',\n",
       "  'h ni nghi',\n",
       "  'ha bnh',\n",
       "  'ha bnh v',\n",
       "  'ha dejado',\n",
       "  'ha dejado ms',\n",
       "  'gulfkanawut',\n",
       "  'guidelines with world'],\n",
       " 17: ['khng',\n",
       "  'nam khng',\n",
       "  'vit nam khng',\n",
       "  'cng ng',\n",
       "  'trong',\n",
       "  'trong cng ng',\n",
       "  'ngy',\n",
       "  'khng c',\n",
       "  'nam khng c',\n",
       "  'cng',\n",
       "  'mi trong',\n",
       "  'trong cng',\n",
       "  'vit',\n",
       "  'vit nam',\n",
       "  'nam',\n",
       "  'ngy th',\n",
       "  'mi trong cng',\n",
       "  'c ca covid',\n",
       "  'khng ghi',\n",
       "  'khng ghi nhn'],\n",
       " 18: ['kapeefever',\n",
       "  'n b',\n",
       "  'lolsc',\n",
       "  'sdecojuqdan',\n",
       "  'gusta',\n",
       "  'gusta tu',\n",
       "  'gym',\n",
       "  'gym need',\n",
       "  'h nam',\n",
       "  'h ngay',\n",
       "  'h ngay c',\n",
       "  'h ni',\n",
       "  'gulfeverindia',\n",
       "  'h ni ghi',\n",
       "  'h ni lp',\n",
       "  'h ni nghi',\n",
       "  'ha bnh',\n",
       "  'ha bnh v',\n",
       "  'ha dejado',\n",
       "  'gulfkanawut'],\n",
       " 19: ['beambeamfever',\n",
       "  'beambeamfever mv',\n",
       "  'fansign',\n",
       "  'beambeamfever et',\n",
       "  'shot',\n",
       "  'infinity love',\n",
       "  'janchan',\n",
       "  'infinity',\n",
       "  'love',\n",
       "  'ifyouwantme',\n",
       "  'plaza',\n",
       "  'central plaza',\n",
       "  'central plaza westgate',\n",
       "  'plaza westgate',\n",
       "  'westgate',\n",
       "  'central',\n",
       "  'h ni nghi',\n",
       "  'gym need',\n",
       "  'ha bnh',\n",
       "  'ha bnh v']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topicTermsDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(outPath+\"dataWithEmotionEntityTopic.csv\")\n",
    "with open(outPath+\"topicTermDict.json\",\"w\") as jsonFile:\n",
    "    jsonFile.write(json.dumps(topicTermsDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
